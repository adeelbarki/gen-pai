# ğŸ©º GenPAI â€” Generative Physician AI

**GenPAI** is a multi-component medical AI system designed to assist patients with non-emergency general health questions.  
It combines a modern Angular frontend, a .NET Core API backend, a Python FastAPI microservice powered by OpenAI + LangChain, and Redis for advanced similarity search.

---

## ğŸŒŸ Project Structure

/genpai/
/frontend/ â†’ Angular app (chat UI)
/api-server/ â†’ .NET Core Web API (routing + orchestration)
/llm-server/ â†’ Python FastAPI LLM service (OpenAI + LangChain)
/redis/ â†’ Redis vector store (RediSearch)


---

## ğŸ“¦ Components

### ğŸ–¼ Frontend (Angular)
- Chat UI for users to submit symptoms and queries
- Displays live AI responses, dynamic loading dots, and conversation history
- Connects to the backend API via HTTP

### âš™ Backend API (C# / .NET Core)
- Receives patient queries from the Angular frontend
- Calls Python FastAPI LLM service for generative responses
- Acts as the secure middle layer between frontend and AI logic

### ğŸ§  LLM Service (Python / FastAPI)
- Handles natural language questions and symptom queries
- Uses OpenAI GPT (with LangChain RAG support)
- Connects to Redis vector search for embedding-based retrieval

### ğŸª Redis (Vector Store)
- Stores medical knowledge chunks + embeddings
- Provides fast KNN similarity search (using RediSearch)

---

## ğŸ”‘ Environment & Config

### âœ… Included in version control:
- Angular app source
- .NET Core API (excluding sensitive configs)
- Python LLM FastAPI service (excluding `.env`)
- Example configs

### âŒ Ignored via `.gitignore`:
- `llm-server/.env` â†’ holds your `OPENAI_API_KEY`
- `llm-server/venv/` â†’ Python virtual environment
- `api-server/PhysicianAI.Api/bin/` and `obj/` â†’ .NET build artifacts
- `api-server/PhysicianAI.Api/appsettings.Development.json` â†’ local dev secrets

---

## âš¡ Quick Start

### 1ï¸âƒ£ Frontend (Angular)
```bash
cd frontend
npm install
npm run start
```
### 2ï¸âƒ£ .NET API Server
```bash
cd api-server/PhysicianAI.Api
dotnet run
```
### 3ï¸âƒ£ Python FastAPI LLM Server
```bash
cd llm-server
python -m venv venv
source venv/bin/activate
pip install -r requirements.txt
uvicorn app.main:app --host 0.0.0.0 --port 5000
```
### 4ï¸âƒ£ Redis (with RediSearch)
```bash
docker run -p 6379:6379 redis/redis-stack-server:latest
```

## ğŸ— Architecture Overview
```bash
[ Angular Frontend ] â†’ [ .NET API Server ] â†’ [ Python FastAPI LLM Service ] â†’ [ Redis Vector DB ]
```
- Frontend sends HTTP requests to .NET API
- .NET API orchestrates backend calls, forwards queries to FastAPI
- FastAPI runs LLM + RAG pipeline, retrieves data from Redis
- Response flows back to the user chat window

## ğŸ” Secrets Handling
- `.env` in Python â†’ holds `OPENAI_API_KEY`
- `appsettings.Development.json` in .NET â†’ holds dev environment configs (ignored in Git)
```bash
OPENAI_API_KEY=your-openai-api-key-here
REDIS_HOST=localhost
REDIS_PORT=6379
```
Example appsettings.Development.json:
```json
{
  "Logging": {
    "LogLevel": {
      "Default": "Information"
    }
  },
  "AllowedHosts": "*"
}
```
## âœ¨ Features
âœ… Handles non-emergency symptom Q&A
âœ… Provides dynamic AI chat responses
âœ… Integrates OpenAI GPT + LangChain RAG pipelines
âœ… Uses Redis vector search for embedding queries
âœ… Modular microservice architecture for scalability

## ğŸ“œ License
This project is under development. License terms will be added in a future release.
